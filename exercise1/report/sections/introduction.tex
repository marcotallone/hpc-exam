\documentclass[../main.tex]{subfiles}

\begin{document}

Collective operation in \textit{Message Passing Interface (MPI)}~\cite{mpi} are foundamental building blocks for parallel applications. They are used to exchange data among processes and are often used to implement higher level parallel algorithms. Efficient implementations of these operations that aim to minimize latency and maximize throughput are therefore a must for high performance computing.\\
This report studies the case of the broadcast and reduce collective implementations of the MPI standard by presenting two models for latency prediction. In particular, since the \textit{OpenMPI} library offers the possibility to choose among different algorithms for these operations, this study, together with the default MPI implementation, also considers the \textit{linear}, \textit{chain} and \textit{binary tree} algorithms for the selected operations.\\
The main problem covered in this report is the task of predicting the overall latency of these operations for various message sizes and number of processes in multi-core systems. The measurement of latency times in different conditions has therefore been carried out by running the well-known \textit{OSU Micro-Benchmarks} suite~\cite{osu} on the \textit{ORFEO} cluster of the AREA Science Park in Trieste~\cite{orfeo} by testing different processes allocations and message sizes.\\
With the collected data, two different models for latency estimation have been built. The first one is based on point-to-point communications latencies and builds on top of the model presented by \textit{Nuriyev et al.}~\cite{Nuriyev2022}. The second model is based on a linear regression approach and uses the collected data to train a model that can predict the latency of the collective operations.\\
The underlying goal of this project has been to compare the two approaches and attempt to find the best compromise between accuracy and affidability in predictions and the overall explainability of the model, taking into account the architecture on which the benchmark has been conducted.\\
The following sections are organized as follows. In Section~\ref{algorithms}, the studied algorithms for the collective operations will be presented and analyzed from a theoretical point of view. Section~\ref{architecture} describes the architecture and the precise methodology used for the measurements. Sections~\ref{model} and~\ref{linear} will then present the implemented models for the latency predictions, followed by the obtained results and some final considerations in the last two sections of this report.\\
% In the following sections, the studied algorithms for the collective operations will be presented, followed by the description of the implemented models for latency prediction. The precise methodology for the data collection and the results obtained with the implemented models will then be presented, toghether with some final considerations.\\
With the aim of maintaining a clear and concise report of the work, the following sections will only present the most noticeable results of the study, while further analysis and the detailed model implementations will be available on the author's GitHub repository~\cite{github}.
% // TODO: Add a reference to the GitHub repository

\end{document}